{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Required Packages\n",
        "\n",
        "### Note: Resolving `keras.src` Namespace Issue\n",
        "When using TensorFlow and TensorFlow Model Optimization in Colab, you may encounter a `keras.src` namespace issue, causing incompatibility with `tensorflow_model_optimization.quantization.keras`. To resolve this:\n",
        "\n",
        "1. Set the `KERAS_BACKEND` environment variable to `tensorflow` before importing TensorFlow.\n",
        "2. Ensure you are using compatible versions of TensorFlow (`>=2.12`) and TensorFlow Model Optimization.\n",
        "3. Clone the model using `tensorflow.keras.models.clone_model()` to ensure it aligns with the `tensorflow.keras` namespace.\n",
        "4. Always restart the runtime and reinstall TensorFlow-related packages to avoid lingering conflicts.\n",
        "\n",
        "This ensures that all operations use the correct `tensorflow.keras` implementation, avoiding compatibility issues.\n"
      ],
      "metadata": {
        "id": "08zHoHPwV_Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y keras tensorflow tensorflow-model-optimization\n",
        "!pip install tensorflow==2.12 tensorflow-model-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w0aZHRPVWGF7",
        "outputId": "57b386b9-c476-415c-f1ba-7274246d8cfe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: keras 3.5.0\n",
            "Uninstalling keras-3.5.0:\n",
            "  Successfully uninstalled keras-3.5.0\n",
            "Found existing installation: tensorflow 2.17.1\n",
            "Uninstalling tensorflow-2.17.1:\n",
            "  Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[33mWARNING: Skipping tensorflow-model-optimization as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tensorflow==2.12\n",
            "  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting tensorflow-model-optimization\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (24.3.25)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.68.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (3.12.1)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (0.4.33)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (18.1.1)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12)\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.16.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (0.37.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (0.1.8)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.33,>=0.4.33 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.33)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.1)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.36,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.36-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.34-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.31-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.30-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12) (1.13.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.2.2)\n",
            "Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jaxlib-0.4.30-cp310-cp310-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, numpy, keras, gast, tensorflow-model-optimization, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.0\n",
            "    Uninstalling wrapt-1.17.0:\n",
            "      Successfully uninstalled wrapt-1.17.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.33\n",
            "    Uninstalling jaxlib-0.4.33:\n",
            "      Successfully uninstalled jaxlib-0.4.33\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.33\n",
            "    Uninstalling jax-0.4.33:\n",
            "      Successfully uninstalled jax-0.4.33\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.27.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.12.0 which is incompatible.\n",
            "xarray 2024.10.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 numpy-1.23.5 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-model-optimization-0.8.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "d6d792b509774cbc88ecf73044b244a6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
      ],
      "metadata": {
        "id": "GNfEPNO4WwV0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part I: Model Pruning with Sparsity\n",
        "This part of the notebook demonstrates how pruning can be used to reduce the size of a model by removing insignificant weights. However, pruning alone does not always lead to size reductions unless additional steps are taken:\n",
        "1. Remove the pruning mask to finalize the pruned model.\n",
        "2. Use TensorFlow Lite's **experimental sparsity-aware optimization** when converting to TFLite format.\n",
        "\n",
        "## Objectives:\n",
        "1. Compare model size before and after removing the pruning mask.\n",
        "2. Show the impact of experimental sparsity optimization on reducing the TFLite model size.\n"
      ],
      "metadata": {
        "id": "urz4uV3kTCBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation\n",
        "We use the MNIST dataset, which contains grayscale images of handwritten digits (0-9).\n",
        "1. Normalize the pixel values to the range [0, 1].\n",
        "2. Reshape the data for input into the CNN model.\n",
        "3. One-hot encode the labels for classification."
      ],
      "metadata": {
        "id": "Ne2XWdVjTZLm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSDB1IRnJ5BP",
        "outputId": "9d6c5cdc-2762-4d3f-ff58-123927274337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Training data shape: (60000, 28, 28, 1), Labels shape: (60000, 10)\n",
            "Test data shape: (10000, 28, 28, 1), Labels shape: (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "print(f\"Training data shape: {x_train.shape}, Labels shape: {y_train.shape}\")\n",
        "print(f\"Test data shape: {x_test.shape}, Labels shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Simple CNN\n",
        "We build a Convolutional Neural Network (CNN) with the following layers:\n",
        "1. **Convolutional Layer**: Extracts features from the input images.\n",
        "2. **MaxPooling Layer**: Reduces spatial dimensions, lowering computational requirements.\n",
        "3. **Flatten Layer**: Converts the 2D feature maps into a 1D vector.\n",
        "4. **Dense Layers**: Fully connected layers for classification.\n",
        "\n",
        "The model is compiled using the Adam optimizer and trained for 2 epochs.\n"
      ],
      "metadata": {
        "id": "cQVFmR1RTbvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a simple CNN model\n",
        "def create_cnn_model():\n",
        "    inputs = Input(shape=(28, 28, 1))\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    outputs = Dense(10, activation='softmax')(x)\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "# Compile and train the model\n",
        "model = create_cnn_model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=2, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the trained model\n",
        "baseline_accuracy = model.evaluate(x_test, y_test, verbose=0)[1]\n",
        "print(f\"Baseline Model Accuracy: {baseline_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL5KvhW_TYcf",
        "outputId": "92ac5d63-c557-47e1-bbc1-d6939890499d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1875/1875 [==============================] - 69s 36ms/step - loss: 0.1535 - accuracy: 0.9549 - val_loss: 0.0628 - val_accuracy: 0.9791\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 0.0511 - accuracy: 0.9840 - val_loss: 0.0476 - val_accuracy: 0.9841\n",
            "Baseline Model Accuracy: 0.9841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruning the Model\n",
        "We apply pruning using TensorFlow Model Optimization. This process sparsifies the model by removing insignificant weights while maintaining comparable accuracy.\n"
      ],
      "metadata": {
        "id": "JhYd6u5ETkfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_model_optimization.sparsity.keras import (\n",
        "    prune_low_magnitude,\n",
        "    PolynomialDecay,\n",
        "    UpdatePruningStep\n",
        ")\n",
        "\n",
        "# Apply pruning to the model\n",
        "def apply_pruning(model):\n",
        "    pruning_params = {\n",
        "        'pruning_schedule': PolynomialDecay(\n",
        "            initial_sparsity=0.5,\n",
        "            final_sparsity=0.9,\n",
        "            begin_step=0,\n",
        "            end_step=2000\n",
        "        )\n",
        "    }\n",
        "    pruned_model = prune_low_magnitude(model, **pruning_params)\n",
        "    return pruned_model\n",
        "\n",
        "# Compile and train the pruned model\n",
        "pruned_model = apply_pruning(create_cnn_model())\n",
        "pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Include the UpdatePruningStep callback\n",
        "callbacks = [UpdatePruningStep()]\n",
        "\n",
        "# Train the pruned model\n",
        "pruned_model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=2,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=callbacks  # Add the required callback\n",
        ")\n",
        "\n",
        "# Evaluate the pruned model\n",
        "pruned_accuracy = pruned_model.evaluate(x_test, y_test, verbose=0)[1]\n",
        "print(f\"Pruned Model Accuracy: {pruned_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aByTko35TjmY",
        "outputId": "c7fa6de1-5840-4e21-efd3-86210beb3597"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1875/1875 [==============================] - 62s 31ms/step - loss: 0.2278 - accuracy: 0.9337 - val_loss: 0.1293 - val_accuracy: 0.9626\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.1086 - accuracy: 0.9676 - val_loss: 0.0948 - val_accuracy: 0.9695\n",
            "Pruned Model Accuracy: 0.9695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Pruned Model Without Removing the Pruning Mask\n",
        "Pruning introduces a pruning mask to track sparse connections. If we save the model without removing the mask, the model size does not reduce significantly.\n"
      ],
      "metadata": {
        "id": "e9QC_tWDTlyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the pruned model without removing the pruning mask\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
        "tflite_model_with_mask = converter.convert()\n",
        "\n",
        "# Save the TFLite model\n",
        "with open(\"pruned_model_with_mask.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_with_mask)\n",
        "\n",
        "print(\"Saved pruned model with pruning mask.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVTsDj0PTMGI",
        "outputId": "a971c4aa-b7a5-473f-90b0-6238a4ec5ac5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, flatten_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pruned model with pruning mask.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing the Pruning Mask and Using Sparsity Optimization\n",
        "To reduce the model size, we must:\n",
        "1. Strip the pruning mask using `strip_pruning`.\n",
        "2. Use TensorFlow Lite's experimental sparsity-aware optimization.\n"
      ],
      "metadata": {
        "id": "F9h5PQUgYrqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
        "\n",
        "# Strip the pruning mask\n",
        "stripped_model = strip_pruning(pruned_model)\n",
        "\n",
        "# Convert the stripped model with experimental sparsity optimization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(stripped_model)\n",
        "converter.optimizations = [tf.lite.Optimize.EXPERIMENTAL_SPARSITY]\n",
        "tflite_model_sparse = converter.convert()\n",
        "\n",
        "# Save the optimized sparse TFLite model\n",
        "with open(\"pruned_model_sparse.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_sparse)\n",
        "\n",
        "print(\"Saved pruned model with experimental sparsity optimization.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T95ShBe3Yw8J",
        "outputId": "08852984-5b13-45b3-bf88-3b4718861a77"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pruned model with experimental sparsity optimization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Model Sizes and Accuracy\n",
        "We compare the sizes of:\n",
        "1. The baseline model.\n",
        "2. The pruned model without removing the pruning mask.\n",
        "3. The pruned model with experimental sparsity optimization.\n"
      ],
      "metadata": {
        "id": "-_adOYtnZBHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Compare model sizes\n",
        "model_files = [\n",
        "    \"pruned_model_with_mask.tflite\",\n",
        "    \"pruned_model_sparse.tflite\"\n",
        "]\n",
        "\n",
        "print(\"\\nModel Sizes (KB):\")\n",
        "for file in model_files:\n",
        "    print(f\"{file}: {os.path.getsize(file) / 1024:.2f} KB\")\n",
        "\n",
        "# Evaluate accuracy for sparse TFLite model\n",
        "def evaluate_tflite_model(tflite_model_path):\n",
        "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    correct_predictions = 0\n",
        "    for i in range(len(x_test)):\n",
        "        input_data = x_test[i:i+1].astype(\"float32\")\n",
        "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "        interpreter.invoke()\n",
        "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "        if tf.argmax(output_data, axis=1) == tf.argmax(y_test[i:i+1], axis=1):\n",
        "            correct_predictions += 1\n",
        "\n",
        "    return correct_predictions / len(x_test)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"\\nModel Accuracies:\")\n",
        "print(f\"Pruned Model with Mask Accuracy: {pruned_accuracy:.4f}\")\n",
        "print(f\"Pruned Sparse Model Accuracy: {evaluate_tflite_model('pruned_model_sparse.tflite'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k42uW-A3ZBRO",
        "outputId": "82e58f8c-e4cc-4e87-f29d-66b7b132eac5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Sizes (KB):\n",
            "pruned_model_with_mask.tflite: 5427.45 KB\n",
            "pruned_model_sparse.tflite: 411.48 KB\n",
            "\n",
            "Model Accuracies:\n",
            "Pruned Model with Mask Accuracy: 0.9695\n",
            "Pruned Sparse Model Accuracy: 0.9695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the baseline (unpruned) model to TensorFlow Lite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model_baseline = converter.convert()\n",
        "\n",
        "# Save the baseline TFLite model\n",
        "with open(\"baseline_model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_baseline)\n",
        "\n",
        "# Print the size of the baseline model\n",
        "import os\n",
        "baseline_model_size = os.path.getsize(\"baseline_model.tflite\") / 1024  # Size in KB\n",
        "print(f\"Baseline Model Size: {baseline_model_size:.2f} KB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1unLQRtZvx1",
        "outputId": "fa46a159-53dd-4809-aa0a-32f1dc9d0b28"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Model Size: 2713.34 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part I Summary\n",
        "- Saving the pruned model without removing the pruning mask does not reduce the size.\n",
        "- Using experimental sparsity-aware optimization after stripping the pruning mask significantly reduces the model size while maintaining accuracy.\n",
        "- Pruning combined with sparsity is highly effective for compressing models for deployment on resource-constrained devices.\n"
      ],
      "metadata": {
        "id": "6QSjm0MUatVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II: Model Pruning + Quantization\n",
        "\n",
        "### Overview\n",
        "In this section, we combine **model pruning** and **quantization** to optimize a neural network for deployment on resource-constrained devices.\n",
        "\n",
        "- **Pruning** removes insignificant connections (weights) in the model, introducing sparsity, which can reduce model size and computational requirements.\n",
        "- **Quantization** reduces the precision of weights and activations, further compressing the model and enabling efficient inference.\n",
        "\n",
        "### Goals\n",
        "1. Apply pruning to the model to introduce sparsity.\n",
        "2. Explore the impact of:\n",
        "   - Saving the pruned model **without removing the pruning mask**.\n",
        "   - Removing the pruning mask and enabling sparsity-aware optimizations.\n",
        "3. Quantize the pruned model using **Full Integer Quantization** and measure its impact on:\n",
        "   - Model size.\n",
        "   - Accuracy on the test set.\n",
        "\n",
        "### Key Points\n",
        "- **Pruning Masks:**\n",
        "  Pruning introduces masks to track sparse connections in the model. These masks must be removed before final deployment to reduce size.\n",
        "  \n",
        "- **Sparsity-Aware Optimization:**\n",
        "  TensorFlow Lite’s `EXPERIMENTAL_SPARSITY` optimization leverages the sparse structure of pruned models to significantly reduce storage and computation.\n",
        "\n",
        "- **Quantization:**\n",
        "  Integer quantization further reduces the model's size and enables inference on hardware that supports integer arithmetic, such as microcontrollers.\n",
        "\n",
        "### Steps Demonstrated\n",
        "1. Prune the model and save it **with the pruning mask**.\n",
        "2. Remove the pruning mask and apply **sparsity-aware optimization**.\n",
        "3. Quantize the pruned models (both with and without the mask) to **Full Integer Quantization**.\n",
        "4. Compare the model sizes and test accuracies across:\n",
        "   - Pruned Model with Mask\n",
        "   - Pruned Model with Sparsity Optimization\n",
        "   - Fully Integer Quantized Models"
      ],
      "metadata": {
        "id": "uqja0olwgV1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Integer Quantization for Pruned Model with Mask\n",
        "\n",
        "def representative_data_gen():\n",
        "    for input_value in x_test[:100]:  # Use a subset of the test set\n",
        "        # Yield a dictionary where the key matches the model's input tensor name\n",
        "        yield [input_value.reshape(1, 28, 28, 1).astype(\"float32\")]\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen  # Use the same representative dataset generator\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "tflite_model_with_mask_int8 = converter.convert()\n",
        "\n",
        "# Save the quantized model\n",
        "with open(\"pruned_model_with_mask_int8.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_with_mask_int8)\n",
        "print(\"Saved Full Integer Quantized Model (with Mask).\")\n",
        "\n",
        "# Full Integer Quantization for Stripped Model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(stripped_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT, tf.lite.Optimize.EXPERIMENTAL_SPARSITY]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "tflite_model_sparse_int8 = converter.convert()\n",
        "\n",
        "# Save the quantized model\n",
        "with open(\"pruned_model_sparse_int8.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model_sparse_int8)\n",
        "print(\"Saved Full Integer Quantized Model (Stripped).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5tfiR7Ibbrt",
        "outputId": "fa3bbc44-cf9f-4511-a3a3-dfa2cc137341"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, flatten_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:789: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Full Integer Quantized Model (with Mask).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:789: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Full Integer Quantized Model (Stripped).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare model sizes after full integer quantization\n",
        "quantized_model_files = [\n",
        "    \"pruned_model_with_mask_int8.tflite\",\n",
        "    \"pruned_model_sparse_int8.tflite\"\n",
        "]\n",
        "\n",
        "print(\"\\nModel Sizes After Full Integer Quantization (KB):\")\n",
        "for file in quantized_model_files:\n",
        "    print(f\"{file}: {os.path.getsize(file) / 1024:.2f} KB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvfNEgj0bwGB",
        "outputId": "f2c63113-e04a-43dc-a057-9fb0319b588d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Sizes After Full Integer Quantization (KB):\n",
            "pruned_model_with_mask_int8.tflite: 3397.02 KB\n",
            "pruned_model_sparse_int8.tflite: 209.93 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_tflite_model(tflite_model_path):\n",
        "    # Load the TFLite model\n",
        "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Get input and output tensor details\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    # Extract input type and scale/zero point for quantization\n",
        "    input_type = input_details[0]['dtype']\n",
        "    input_scale, input_zero_point = input_details[0]['quantization']\n",
        "\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for i in range(len(x_test)):\n",
        "        # Prepare the input data based on quantization parameters\n",
        "        input_data = x_test[i:i+1].astype(\"float32\")  # Original test input (float32)\n",
        "\n",
        "        # Quantize if necessary\n",
        "        if input_type == np.uint8 or input_type == np.int8:\n",
        "            input_data = (input_data / input_scale + input_zero_point).astype(input_type)\n",
        "\n",
        "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "        interpreter.invoke()\n",
        "\n",
        "        # Get the output data\n",
        "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "        # Dequantize the output if necessary\n",
        "        output_scale, output_zero_point = output_details[0]['quantization']\n",
        "        if output_scale != 0:  # Quantized output\n",
        "            output_data = output_scale * (output_data - output_zero_point)\n",
        "\n",
        "        # Compare predictions\n",
        "        if np.argmax(output_data) == np.argmax(y_test[i]):\n",
        "            correct_predictions += 1\n",
        "\n",
        "    return correct_predictions / len(x_test)\n",
        "\n",
        "\n",
        "# Evaluate accuracy for quantized TFLite models\n",
        "#accuracy_with_mask = evaluate_tflite_model(\"pruned_model_with_mask_int8.tflite\")\n",
        "accuracy_sparse = evaluate_tflite_model(\"pruned_model_sparse_int8.tflite\")\n",
        "\n",
        "#print(f\"Pruned Model with Mask Accuracy: {accuracy_with_mask:.4f}\")\n",
        "print(f\"Pruned Sparse Model Accuracy: {accuracy_sparse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytCAXIwvhLOj",
        "outputId": "ccd24447-b529-4095-f819-8a87a66ec6c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned Sparse Model Accuracy: 0.9691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II Summary\n",
        "- **Model Size Comparison**:\n",
        "  - Full Integer Quantization on the pruned model with the mask does not achieve optimal compression.\n",
        "  - Full Integer Quantization on the stripped sparse model results in a much smaller size due to the removal of the pruning mask and sparsity-aware optimization.\n",
        "\n",
        "- **Accuracy Comparison**:\n",
        "  - Both models achieve similar accuracy on the test set, demonstrating that stripping the pruning mask does not affect performance while optimizing size.\n",
        "\n",
        "This demonstrates the importance of properly finalizing a pruned model for deployment.\n",
        "\n"
      ],
      "metadata": {
        "id": "lMfDvq99ar1s"
      }
    }
  ]
}